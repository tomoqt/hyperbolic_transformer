Summary Statistics:

Standard Model:
  Overall Average SVD Entropy: 6.5044 (log: 0.8132)
  Layer Type Average SVD Entropy:
    attention_proj: 6.3364 (log: 0.8018) from 6 parameters
    attention_qkv: 6.5870 (log: 0.8187) from 6 parameters
    embedding: 6.3422 (log: 0.8022) from 2 parameters
    mlp_fc: 6.5966 (log: 0.8193) from 6 parameters
    mlp_proj: 6.5516 (log: 0.8163) from 6 parameters

  Layer-wise Average SVD Entropy:
    Layer 0: 6.5375 (log: 0.8154)
    Layer 1: 6.5247 (log: 0.8146)
    Layer 2: 6.5236 (log: 0.8145)
    Layer 3: 6.4996 (log: 0.8129)
    Layer 4: 6.5018 (log: 0.8130)
    Layer 5: 6.5202 (log: 0.8143)

Hyperbolic Model:
  Overall Average SVD Entropy: 5.6105 (log: 0.7490)
  Layer Type Average SVD Entropy:
    attention_proj: 6.4265 (log: 0.8080) from 6 parameters
    attention_qkv: 6.5874 (log: 0.8187) from 6 parameters
    embedding: 6.3332 (log: 0.8016) from 2 parameters
    mlp_fc: 6.5897 (log: 0.8189) from 6 parameters
    mlp_proj: 6.5159 (log: 0.8140) from 6 parameters
    other: 1.6919 (log: 0.2284) from 6 parameters

  Layer-wise Average SVD Entropy:
    Layer 0: 5.5312 (log: 0.7428)
    Layer 1: 5.5480 (log: 0.7441)
    Layer 2: 5.5770 (log: 0.7464)
    Layer 3: 5.5709 (log: 0.7459)
    Layer 4: 5.5718 (log: 0.7460)
    Layer 5: 5.5748 (log: 0.7462)

Comparison Statistics:
  Overall Entropy Ratio (Hyperbolic/Standard): 0.8626
  Log10 Difference (Hyperbolic - Standard): -0.0642
  Percent Difference: -13.74%


LOG SCALE SUMMARY:
==================

Standard Model (Log10 Values):
  Overall Average SVD Entropy (log10): 0.8132
  Layer Types by Order of Magnitude (ascending):
    attention_proj: 0.8018 (from 6 parameters)
    embedding: 0.8022 (from 2 parameters)
    mlp_proj: 0.8163 (from 6 parameters)
    attention_qkv: 0.8187 (from 6 parameters)
    mlp_fc: 0.8193 (from 6 parameters)

Hyperbolic Model (Log10 Values):
  Overall Average SVD Entropy (log10): 0.7490
  Layer Types by Order of Magnitude (ascending):
    other: 0.2284 (from 6 parameters)
    embedding: 0.8016 (from 2 parameters)
    attention_proj: 0.8080 (from 6 parameters)
    mlp_proj: 0.8140 (from 6 parameters)
    attention_qkv: 0.8187 (from 6 parameters)
    mlp_fc: 0.8189 (from 6 parameters)
